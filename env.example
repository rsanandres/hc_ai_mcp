# HC-AI MCP Server - Environment Configuration
# Copy this file to .env and update values as needed
#
# cp env.example .env

# =============================================================================
# DATABASE (PostgreSQL + pgvector)
# =============================================================================
DB_HOST=localhost
DB_PORT=5432
DB_NAME=hc_ai
DB_USER=postgres
DB_PASSWORD=your_password_here

# Table configuration
DB_TABLE_NAME=hc_ai_chunks
DB_SCHEMA_NAME=public
DB_VECTOR_SIZE=1024  # Adjust based on embedding model

# Connection pool settings
DB_MAX_POOL_SIZE=10
DB_MAX_OVERFLOW=5
DB_POOL_TIMEOUT=30

# =============================================================================
# EMBEDDINGS
# =============================================================================
# Provider: "ollama" (local) or "bedrock" (AWS)
EMBEDDING_PROVIDER=ollama

# Ollama settings (when EMBEDDING_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBED_MODEL=mxbai-embed-large:latest
OLLAMA_EMBED_BATCH_SIZE=8
OLLAMA_EMBED_MAX_PARALLEL=4

# Bedrock settings (when EMBEDDING_PROVIDER=bedrock)
# AWS_REGION=us-east-1
# BEDROCK_EMBED_MODEL=amazon.titan-embed-text-v1
# BEDROCK_EMBED_BATCH_SIZE=4

# =============================================================================
# LLM (for AI Agent)
# =============================================================================
# Provider: "ollama", "bedrock", "openai", or "anthropic"
LLM_PROVIDER=ollama

# Model settings
LLM_MODEL=llama3
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
LLM_NUM_CTX=4096  # Ollama context window

# OpenAI settings (when LLM_PROVIDER=openai)
# OPENAI_API_KEY=your_openai_api_key
# OPENAI_MODEL=gpt-4o-mini

# Anthropic settings (when LLM_PROVIDER=anthropic)
# ANTHROPIC_API_KEY=your_anthropic_api_key
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Bedrock settings (when LLM_PROVIDER=bedrock)
# AWS_REGION=us-east-1

# =============================================================================
# DEBUGGING / LOGGING
# =============================================================================
# Enable verbose logging: "true" or "false"
HC_AI_DEBUG=false

# =============================================================================
# TIMEOUTS
# =============================================================================
AGENT_TIMEOUT=60
RERANK_TIMEOUT=30

# =============================================================================
# RERANKER
# =============================================================================
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_DEVICE=auto  # "auto", "cpu", or "cuda"

# =============================================================================
# CACHE
# =============================================================================
CACHE_TTL=3600  # Seconds
CACHE_MAX_SIZE=10000  # Max entries

# =============================================================================
# CHUNK QUEUE (for background processing)
# =============================================================================
CHUNK_QUEUE_MAX_SIZE=1000
CHUNK_MAX_RETRIES=5
CHUNK_RETRY_BASE_DELAY=1.0
CHUNK_RETRY_MAX_DELAY=60.0

# =============================================================================
# SESSION STORAGE
# =============================================================================
# Provider: "memory" (in-memory) or "dynamodb" (AWS)
SESSION_PROVIDER=memory
SESSION_RECENT_LIMIT=10
SESSION_TTL_DAYS=7
SESSION_MAX_SESSIONS=1000
SESSION_CLEANUP_INTERVAL_SECONDS=300

# DynamoDB settings (when SESSION_PROVIDER=dynamodb)
# AWS_REGION=us-east-1
# DDB_TURNS_TABLE=hcai_session_turns
# DDB_SUMMARY_TABLE=hcai_session_summary
# DDB_ENDPOINT=  # Optional: for local DynamoDB
# DDB_AUTO_CREATE=false  # Auto-create tables

# =============================================================================
# AGENT
# =============================================================================
AGENT_MAX_ITERATIONS=10
