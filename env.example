# HC-AI MCP Server - Environment Configuration
# Copy this file to .env and update values as needed
#
# cp env.example .env

# =============================================================================
# DATABASE (PostgreSQL + pgvector)
# =============================================================================
DB_HOST=localhost
DB_PORT=5432
DB_NAME=hc_ai
DB_USER=postgres
DB_PASSWORD=your_password_here

# Table configuration
DB_TABLE_NAME=hc_ai_chunks
DB_SCHEMA_NAME=public
DB_VECTOR_SIZE=1024  # Must match embedding model dimensions

# Connection pool settings
DB_MAX_POOL_SIZE=10
DB_MAX_OVERFLOW=5
DB_POOL_TIMEOUT=30

# SSL / RDS (auto-detected for remote hosts)
# DB_SSL_ENABLED=auto       # "auto", "true", "false"
# DB_SSL_CERT=              # Path to CA cert, or "rds" to auto-download AWS RDS cert

# IVFFlat index tuning (0 = use PostgreSQL default)
# DB_IVFFLAT_PROBES=0

# =============================================================================
# EMBEDDINGS
# =============================================================================
# Provider: "ollama" (local), "bedrock" (AWS), or "nomic" (API)
EMBEDDING_PROVIDER=ollama

# Ollama settings (when EMBEDDING_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBED_MODEL=mxbai-embed-large:latest
OLLAMA_EMBED_BATCH_SIZE=8

# Bedrock settings (when EMBEDDING_PROVIDER=bedrock)
# AWS_REGION=us-east-1
# BEDROCK_EMBED_MODEL=amazon.titan-embed-text-v2:0
# BEDROCK_EMBED_BATCH_SIZE=4

# Bedrock timeouts (shared by LLM and embedding Bedrock clients)
# BEDROCK_READ_TIMEOUT=60
# BEDROCK_CONNECT_TIMEOUT=10
# BEDROCK_MAX_RETRIES=2

# =============================================================================
# LLM (for AI Agent)
# =============================================================================
# Provider: "ollama", "bedrock", "openai", or "anthropic"
LLM_PROVIDER=ollama

# Model settings
LLM_MODEL=llama3
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
LLM_NUM_CTX=4096           # Ollama context window
LLM_TIMEOUT_SECONDS=60     # Ollama call timeout

# OpenAI settings (when LLM_PROVIDER=openai)
# OPENAI_API_KEY=your_openai_api_key
# LLM_MODEL=gpt-4o

# Anthropic settings (when LLM_PROVIDER=anthropic)
# ANTHROPIC_API_KEY=your_anthropic_api_key
# LLM_MODEL=claude-sonnet-4-20250514

# Bedrock settings (when LLM_PROVIDER=bedrock)
# AWS_REGION=us-east-1
# LLM_MODEL=haiku           # Friendly names: "sonnet", "haiku", "opus", or full model ID

# =============================================================================
# AGENT
# =============================================================================
AGENT_MAX_ITERATIONS=10
AGENT_TIMEOUT=120           # Full agent execution timeout (seconds)
AGENT_GRAPH_TYPE=simple     # Agent graph routing type
# AGENT_PROMPTS_FILE=prompts.yaml
# ENABLE_SESSION_HISTORY=false

# =============================================================================
# RERANKER
# =============================================================================
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_DEVICE=auto        # "auto", "cpu", or "cuda"
RERANK_TIMEOUT=30

# =============================================================================
# CACHE
# =============================================================================
CACHE_TTL=3600              # Seconds
CACHE_MAX_SIZE=10000        # Max entries

# =============================================================================
# CHUNK QUEUE (background processing)
# =============================================================================
CHUNK_QUEUE_MAX_SIZE=1000
CHUNK_MAX_RETRIES=5
CHUNK_RETRY_BASE_DELAY=1.0
CHUNK_RETRY_MAX_DELAY=60.0

# =============================================================================
# SESSION STORAGE
# =============================================================================
# Provider: "memory" (in-memory, default) or "dynamodb" (AWS)
SESSION_PROVIDER=memory
SESSION_RECENT_LIMIT=10

# DynamoDB settings (when SESSION_PROVIDER=dynamodb)
# AWS_REGION=us-east-1
# DDB_TURNS_TABLE=hcai_session_turns
# DDB_SUMMARY_TABLE=hcai_session_summary
# DDB_ENDPOINT=http://localhost:8001
# DDB_AUTO_CREATE=false
# DDB_TTL_DAYS=7

# =============================================================================
# PII MASKING
# =============================================================================
# Provider: "local" (regex-based) or "aws" (Comprehend Medical)
PII_MASKER_PROVIDER=local
GUARDRAILS_ENABLED=true

# =============================================================================
# MEDICAL / TERMINOLOGY APIs (optional, public APIs work without keys)
# =============================================================================
# NCBI_API_KEY=              # Increases rate limits for PubMed searches
# LOINC_USERNAME=            # Required for LOINC lookups
# LOINC_PASSWORD=
# OPENFDA_API_KEY=           # Increases rate limits for FDA drug queries

# API base URLs (override only if using proxies)
# NCBI_API_URL=https://eutils.ncbi.nlm.nih.gov/entrez/eutils
# CLINICAL_TRIALS_API_URL=https://clinicaltrials.gov/api/v2/studies
# WHO_GHO_URL=https://ghoapi.azureedge.net/api
# ICD10_API_URL=https://clinicaltables.nlm.nih.gov/api/icd10cm/v3
# RXNORM_API_URL=https://rxnav.nlm.nih.gov/REST
# OPENFDA_BASE_URL=https://api.fda.gov
# OPENFDA_LABEL_URL=https://api.fda.gov/drug/label.json

# =============================================================================
# DEBUGGING / LOGGING
# =============================================================================
HC_AI_DEBUG=false
